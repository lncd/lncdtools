{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>lncdtools is a suit of shell scripting, GNU Make, and general neuroimaging <sup>1</sup> companion tools developed in the Laboratory of NeuroCognitive Development.</p> <p>Also see LNCDR for GNU R functions. </p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>setup - guide for \"installing\": clone and add to path</li> <li><code>tat2</code> - documentation for calculate time average T2* on 4D EPI</li> <li>BIDS - converting DICOM folders to a BIDS spec file hierarchy using <code>dcmdirtab</code>, <code>dcmtab_bids</code>, and <code>mknii</code></li> <li>shell scripting tools - docs for shell scripting with <code>iffmain</code>,  <code>waitforjobs</code>, <code>dryrun</code>, <code>drytee</code></li> <li><code>niinote</code> - warp a nifti creating command to append AFNI's note header (ad hoc provenance)</li> <li><code>mkls</code>, <code>mkifdiff</code>, <code>mkstat</code>, <code>mkmissing</code> - tools for using make with sentinel files</li> <li><code>3dDeconLogGLTs</code> <code>3dmaskave_grp</code> <code>3dMinStdClust</code> <code>3dNotes_each</code> <code>3dSeedCorr</code> <code>4dConcatDataTable</code> <code>4dConcatSubBriks</code> - afni extensions</li> </ul> <ol> <li> <p>mostly via AFNI \u21a9</p> </li> </ol>"},{"location":"BIDS/","title":"BIDS","text":"<p>The Brain Imaging Data Struture standard provides a consistant organiziation for MRI data. There are many \"converters\" to translate dicom scanner files into a BIDS conforming file hiearchy. This is one.</p> <p>Jump to the example</p>"},{"location":"BIDS/#alternative-converters","title":"Alternative Converters","text":"<p><code>heudiconv</code> and <code>dcm2bids</code> are more popular and better supported tools. The advantage the tools presented here have are within a narrow use-case.</p> <ul> <li><code>dcmdirtab</code> is faster because it does less (at the cost of robustness).</li> <li><code>dcmtab_bids</code> provides a small domain specific language for assigning dicom folders a modality that may be easier to grok and more ergonomic for well known modalities.</li> <li>multiple tools allow for a more inspectible pipeline that is easier to tweak. <sup>1</sup></li> <li>scripts contain few lines of code and can be tweaked for specific unusual input/output.</li> </ul>"},{"location":"BIDS/#overview","title":"Overview","text":"<p>The route from Dicom to BIDS can be broken into 3 </p> <ol> <li>extracting imaging metadata (file name, counts, dicom header)</li> <li>translating metadata to BIDS files names</li> <li>Converting images</li> </ol>"},{"location":"BIDS/#example","title":"Example","text":"<p>Below converts all the dicom folders in <code>*/DICOM/*</code> to BIDS inside <code>bids/</code></p> <ul> <li><code>dcmdb.tsv</code> will store the dicom metadata <sup>2</sup></li> <li><code>bidsname_dcmname.tsv</code> has the dicom folder to bids file conversion <sup>2</sup></li> <li><code>bids/</code> stores the newly created files</li> </ul> bids to dicom in 3 steps<pre><code>dcmdirtab -s '(?&lt;=/)[a-z]{3}' -d '*/DICOM/*' &gt; dcmdb.tsv  # (1)!\n\ndcmtab_bids \\\n 'T1w;dname=UNI-DEN,ndcm=192' \\\n 'bold=rest;pname=Rest,ndcm=300;2' \\\n 'MTR;pname=gre_NM_fa250_t14p4_df2p8khz;acq=NM' \\\n &lt; dcmdb.tsv &gt; bidsname_dcmname.tsv\n\nparallel --colsep '\\t' mknii \"bids/{1}\" \"{2}\" &lt; bidsname_dcmname.tsv\n</code></pre> <ol> <li>matching subject names like 'abc' or 'xyz' using look behind <code>(?&lt;=blah)</code> in a regular expressions. These are especially useful in finding names at the start of a directory but not capturing the leading <code>/</code>.</li> </ol> <p>One of the many conversion this might complete:</p> <p><code>abc/DICOM/GRE_NM_FA250_T14P4_DF2P8KHZ_0026</code> =&gt; <code>sub-abc/anat/sub-abc_acq-NM_MTR.nii.gz</code></p>"},{"location":"BIDS/#tools","title":"Tools","text":""},{"location":"BIDS/#metadata-dcmdirtab","title":"metadata: <code>dcmdirtab</code>","text":"<p><code>dcmdirtab</code> outputs dicom directory metadata as tab separated fields.</p> <p>It matches folders from the glob provided by <code>-d</code>, extracts sub id with pattern given after <code>-s</code> and session patter after <code>-b</code>. additional columns can be added and even defined with other arguments.</p> <p>This script defaults to running quickly. Unlike more robust converters, it does not inspect every dicom. Instead the first in a folder is picked as representative. This has drawbacks: multiple echos, TRs, and TEs are not tracked. This hasn't caused known problems yet.</p>"},{"location":"BIDS/#fields","title":"Fields","text":"<p>The default output has the header is <pre><code>subj    seqno   ndcm    pname   tr      matrix  acqdir  dname   fullpath\n</code></pre></p> <p>But there are many pre-built fields that can be extracted</p> <pre><code> dcmdirtab -l\nBuilt in columns:\n        fullpath =&gt; CODE(0x555555d046e8)\n        pname =&gt; 0018,1030\n        dname =&gt; CODE(0x55555620ba58)\n        inverttime =&gt; 0018,0082\n        seriesdesc =&gt; 0008,103e\n        station =&gt; 0008,1010\n        tr =&gt; 0018,0080\n        subj =&gt; (?^u:\\d+)\n        seqname =&gt; 0018,0024\n        matrix =&gt; 0018,1310\n        example =&gt; CODE(0x555556184670)\n        acqdir =&gt; 0018,1312\n        model =&gt; 0008,1090\n        software =&gt; 0018,1020\n        imagetime =&gt; 0008,0033\n        accel =&gt; CODE(0x555555d03ef0)\n        ndcm =&gt; CODE(0x5555559cf8e8)\n        bandwidth =&gt; 0018,0095\n        ses =&gt; (?^u:)\n        flipangle =&gt; 0018,1314\n        acqdir_alt =&gt; 0051,100e\n        strength =&gt; 0018,0087\n        echotime =&gt; 0018,0081\n        seqno =&gt; 0020,0011\n        patname =&gt; 0010,0010\n</code></pre> <p>using <code>-b 'sessionpattern'</code> implicitly adds the <code>ses</code> column</p> <p>see <code>dcmdirtab --help</code> for much more.</p>"},{"location":"BIDS/#naming-dcmtab_bids","title":"naming: <code>dcmtab_bids</code>","text":""},{"location":"BIDS/#input","title":"Input","text":"<p>STDIN input is a tsv with the first row as a header, exactly what <code>dcmdirtab</code> outputs. If <code>ses</code> is a column, output will include that in BIDS names (<code>sub-xxx/ses-yyy/*/sub-xxx_ses-yyy_*</code>)</p> <p>Provided arguments to the command are sets of specifications to identify modalities in the dicom metadata and translate to BIDS compliant names.</p> <pre><code>mode;pattern,pattern[;runs][;acq=label][;dir=label]\n</code></pre> <p>Though patterns use <code>=</code>, they actually are using a regular expression to match strings.</p> <p><code>pname=Rest,ndcm=300</code> matches any protocol with \"Rest\" in the name. and if there were 3000 dicoms, it'd match that the same as if there are 300 (b/c \"300\" is in the string \"3000\").</p>"},{"location":"BIDS/#output","title":"Output","text":"<p>output is tab separated fields: (1) file name in bids (2) input dicom directory. like</p> <pre><code>sub-abc/anat/sub-abc_acq-NM_MTR.nii.gz    abc/DICOM/GRE_NM_FA250_T14P4_DF2P8KHZ_0026\n...\nsub-xxx/anat/sub-xxx_T1w.nii.gz   xxx/DICOM/MP2RAGEPTX_TR6000_1MMISO_UNI-DEN_0034\n</code></pre> <p>see <code>dcmtab_bids --help</code></p>"},{"location":"BIDS/#create-mknii","title":"create: <code>mknii</code>","text":"<p><code>mknii</code> takes a dicom folder and an ideal output name on STDIN, exactly what <code>dcmtab_bids</code> outputs.</p> <p>It recklessly makes whatever folder it's told is needed, uses <code>dcm2niix</code> to write <code>*.nii.gz</code>+<code>.json</code> pairs, and renames them to the given output name.</p> <p><code>3dNotes</code> is used to record the command within the nifti file for posterity as provenance. NB. if you have PHI/PII in the dicom folder name, this will be pushed into header of the <code>.nii.gz</code> file </p>"},{"location":"BIDS/#lncdtools-with-perl-and-afni-on-hpc","title":"lncdtools with perl and afni on HPC","text":"<p><code>dcmdirtab</code> uses requires a perl &gt;=5.26 in addition to <code>dicom_hinfo</code> form AFNI. For advance usage (evaluating perl configuration file), a library from CPAN <code>File::Slurp</code> is also used.</p> <p>Unfortunately this can complicate setup.  Here's an example script to <code>source</code>, used on Pitt's CRC HPC cluster.</p> <pre><code># use newer perl. as of 2023-09-07, newest on CRC loaded like\nmodule load gcc/8.2.0 perl/5.28.0\n\n# and we need afni\nmodule load afni\n\nexport PATH=\"$PATH:/path/to/cloned/lncdtools\"\n\n# ---\n# this is only need if using `-e config.pl` option of dcmdirtab (unlikley)\n# ---\n#\n# File::Slurp (and implicitly local::lib) only need to be install once\n# cpan is interactive, must pick lib::local (i.e. dont change the defaults)\n! test -d $HOME/perl5/lib/perl5 &amp;&amp;\n        cpan install File::Slurp\n\n# export the environment perl needs to know where lib::local stuff lives\neval $(PERL5LIB=\"$HOME/perl5/lib/perl5\" perl -Mlocal::lib)\n</code></pre> <ol> <li> <p>do one thing well \u21a9</p> </li> <li> <p>alternatively, we could forgo saving files and pipe from one command directly to the next   <code>dcmdirtab ... | dcmtab_bids .... | parallel ... mknii ...</code> \u21a9\u21a9</p> </li> </ol>"},{"location":"colophon/","title":"Colophon","text":"<p>Documentation is built with <code>mkdocs-material</code> using github actions (.github/workflows/ci.yml). GitHub Pages setting is to \"deploy from branch\" for \"gh-pages\" on \"/ (root)\".</p>"},{"location":"lncdtools_setup/","title":"Setup","text":""},{"location":"lncdtools_setup/#bleeding-edge","title":"Bleeding edge","text":"<p>The quickest way to get the most up-to-date lncdtools is to clone and add the new directory to your <code>$PATH</code>. Modify your shell login resource file to make the addition more perminate.</p> <pre><code>git clone https://github.com/lncd/lncdtools.git $HOME/lncdtools # (1)\nexport PATH=\"$PATH:$HOME/lncdtools\"\n\necho 'export PATH=\"\\$PATH:$HOME/lncdtools\"' &gt;&gt; $HOME/.bashrc # (2)\n</code></pre> <ol> <li><code>$HOME/lncdtools</code> can be whereever and named whatever. </li> <li><code>$HOME/.bashrc</code> should be what your shell sources on login. it mighte be <code>~/.zshrc</code> on mac or <code>~/.bash_profile</code> elewhere</li> </ol>"},{"location":"lncdtools_setup/#debian","title":"Debian","text":"<p><code>Makefile</code> can create .deb packages for installing on debian and ubuntu. This will hopefully be included on the releases page in the future</p>"},{"location":"lncdtools_setup/#docker","title":"Docker","text":"<p>see <code>Dockerfile</code></p>"},{"location":"lncdtools_setup/#nix","title":"Nix","text":"<p>TODO (last edit: 20230815)</p>"},{"location":"shell/","title":"Shell Tools","text":"<p>Jump right into with an example.</p>"},{"location":"shell/#dryrun","title":"<code>dryrun</code>","text":"<p><code>dryrun</code> runs a command only when the <code>$DRYRUN</code> environmental variable is not set. <sup>1</sup> Also see <code>try</code>, comparable to <code>make -n</code></p> dryrun'ed<pre><code>$ echo hi &gt; myfile\n$ export DRYRUN=1\n$ dryrun rm myfile  \nrm myfile  # (1)!\n$ cat myfile\nhi\n</code></pre> <ol> <li>this is printed but not run</li> </ol> actually run<pre><code>$ echo hi &gt; myfile\n$ export DRYRUN=\n$ dryrun rm myfile # (1)!\n$ cat myfile\ncat: myfile: No such file or directory\n</code></pre> <ol> <li>nothing is printed. <code>rm</code> runs silently as if <code>dryrun</code> was not there</li> </ol> <p>It's worth noting bash allows environmental variables to be set and scoped to a single command by prefacing the call with <code>var=val</code>. For <code>dryrun</code> enabled scripts and functions, this means staring with <code>DRYRUN=1</code> for the \"just print\" version.</p> compact<pre><code>$ example(){ dryrun rm myfile; }\n$ DRYRUN=1 example\nrm myfile # (1)!\n$ echo $DRYRUN\n# (2)!\n</code></pre> <ol> <li><code>rm myfile</code> is printed but not run</li> <li>empty line showing <code>$DRYRUN</code> is not set but was for the call above (where it was explicitly declared)</li> </ol>"},{"location":"shell/#drytee","title":"<code>drytee</code>","text":"<p><code>drytee</code> works like <code>dryrun</code> but for capturing output you may want to be written to a file unless <code>$DRYRUN</code> is set. It's like the command <code>tee</code> but for writing to standard error when the user wants a dry run.</p> <pre><code>$ echo hi | drytee myfile\n$ cat myfile\nhi # (1)!\n$ DRYRUN=1\n$ echo bye | drytee myfile\n#       bye\n# would be written to myfile\n$ cat myfile\nhi # (2)!\n</code></pre> <ol> <li><code>myfile</code> was written (\"hi\") b/c <code>DRYRUN</code> is not set</li> <li><code>myfile</code> is unchanged. <code>bye</code> was not written</li> </ol>"},{"location":"shell/#warn","title":"<code>warn</code>","text":"<p><code>warn</code> could be written <code>echo \"$@\" &gt; &amp;2</code>. It simply writes it's arguments to standard error (2) instead of standard output. This is useful to avoid shell capture to either a variable or a file. avoid capture<pre><code>$ a=$(warn \"oh no\"; echo \"results\")\noh no # (1)!\n$ echo $a\nresults\n</code></pre></p> <ol> <li>'oh no' seen on the terminal b/c it's written to stderr. \"resutls\" on stdout is captured into <code>$a</code></li> </ol> <p>A contrived example for giving a warning that doesn't end up in the output (but still potentially notifies the user) no warning in file<pre><code># create a file of n lines sequentally numbered\nfilelines(){\n  n=\"$1\"\n  [ $n -lt 2 ] &amp;&amp; warn \"# WARNING: n=$n &lt; 2. limited output\"\n  printf \"%s\\n\" $(seq 1 $n)\n}\n</code></pre></p> <pre><code>$ filelines 1 &gt; myfile\n# WARNING: n=1 &lt; 2. limited output\n$ cat myfile\n1\n</code></pre>"},{"location":"shell/#waitforjobs","title":"<code>waitforjobs</code>","text":"<p><code>waitforjobs</code> tracks the number of forked child processes. It waits <code>SLEEPTIME</code> and polls the count until there are fewer than <code>MAXJOBS</code> jobs running. It uses shell job control facilities and is useful for local, single user, or small servers. On HPC, you'd use <code>sbatch</code> from e.g. <code>slurm</code> or <code>torque</code>. Other alternatives include <code>bq</code> and <code>task-spooler</code>. GNU Parallel and Make also have job dispatching facilities.</p> waitforjobs<pre><code>for i in {1..20}; do\n  sleep 5 &amp; # (1)!\n  waitforjobs\ndone\nwait  # (2)!\n</code></pre> <ol> <li><code>sleep</code> here is a stand in for a more useful long running command to be parallelized</li> <li>waitforjobs will exit the final loop with MAXJOBS-1 still running. this <code>wait</code> will wait for those (but wont have the the notifications every SLEEPTIME. could consider <code>waitforjobs -p 1</code> instead.</li> </ol> <p>when running locally, output looks like: <pre><code>2023-05-24T15:38: sleep 60s on 3: sleep 5;sleep 5;bash /home/foranw/src/work/lncdtools/waitforjobs;\n</code></pre></p>"},{"location":"shell/#arguments","title":"Arguments","text":"<pre><code>USAGE:\n  waitforjobs [-j numjobs] [-s sleeptimesecs] [-c \"auto\"]  [-h|--help]\"\n</code></pre> <p><code>-c auto</code> is worth exploring in more detail. Using this option, a temporary file like <code>/tmp/host-user-basename.jobcfg</code> is created. Modifying the sleep and job settings in that file will affect the waitforjobs process watching it. You can change the number of cores to use in real time!</p>"},{"location":"shell/#iffmain","title":"<code>iffmain</code>","text":"<p>In a scripts where <code>main_function</code> is a deifned function, <code>iffmain</code> use at the end like </p> <pre><code>eval \"$(iffmain main_function)\"\n</code></pre> <p>Defensive shell scripting calls for <code>set -euo pipefail</code> but running that (e.g. via <code>source</code>) on the command line will break other scripts and normal interactive shell <sup>2</sup>. <code>iffmain</code> is modeled after the python idiom <code>if __name__ == \"__main__\"</code>. When the script is not sourced, it toggles the ideal settings and sets a standard <code>trap</code> to notify on error.</p>"},{"location":"shell/#sourcing","title":"Sourcing","text":"<p>Using <code>iffmain</code> makes it easier to write bash scripts that are primarily functions. Scripts styled this way are easy to source and test.</p> <p>A bash file that can be sourced can be reused and is able to be tested. See Bash Test Driven Development</p>"},{"location":"shell/#template","title":"Template","text":"<p><code>iffmain</code> generates shell code that looks like iffmain template<pre><code>if [[ \"$(caller)\" == \"0 \"* ]]; then\n  set -euo pipefail\n  trap 'e=$?; [ $e -ne 0 ] &amp;&amp; echo \"$0 exited in error $e\"' EXIT\n  MAINFUNCNAME \"$@\"\n  exit $?\nfi\n</code></pre></p>"},{"location":"shell/#example-script","title":"Example Script","text":"<p>As an example, we'll use <code>drytee</code>, <code>dryrun</code>, and <code>waitforjobs</code> in the script <code>tat2all.bash</code> to</p> <ul> <li>run <code>tat2</code> (<code>tat2_single</code>) on a collection of bold files</li> <li>in parallel (<code>all_parallel</code>) and </li> <li>need to do a few checks (<code>input_checks</code>) before hand.</li> </ul> <p>We'll support </p> <ul> <li>printing what the script would do instead of actually doing it (<code>dryrun</code> and <code>drytee</code>) and</li> <li>using hygienic shell settings (e.g. <code>set -euo pipefail</code>) only when run as a file but not when sourced <sup>3</sup></li> </ul> tat2_all.bash<pre><code>#!/usr/bin/env bash\n\n# create a 1D 0/1 binary censor file based on FD &gt; 0.3mm\ncreate_censor(){\n   mot=${1//bold.nii.gz/motion.txt} # sub*rest_motion.txt\n   out=${1//bold.nii.gz/fdcen.1D}   # sub*rest_fdcen.1D\n   [ ! -r \"$mot\" ] &amp;&amp; warn \"no $mot!\" &amp;&amp; return 1 # (5)!\n   fd_calc 1:3 4:6 deg .3 &lt; \"${mot}\" |\n     drytee \"$out\" # (1)!\n\n   # pass output censor file name so it can be captured\n   echo \"$out\"\n}\n\n# run tat2 for a given bold epi\n# remove high motion timepoints from calculation\ntat2_single(){\n   local input\n   input=\"${1:?input.nii.gz needed}\"\n   out=$(create_censor \"$input\")\n   dryrun tat2 \"$input\" -censor \"$out\" # (2)!\n}\n\n# run tat2 for all bold image files in parallel\ntat2_parallel(){\n  FILES=(sub-*/ses-*/func/*bold.nii.gz)\n\n  for input in \"${FILES[@]}\"; do\n     tat2_single \"$input\" &amp;\n     waitforjobs # (3)!\n     # for testing, just run one using:\n     # break\n  done\n\n  # hold until the final set of jobs to finish\n  wait\n}\n\neval \"$(iffmain tat2_parallel)\" # (4)!\n</code></pre> <ol> <li><code>drytee</code> writes to the specified file unless <code>DRYRUN</code> is set, then it truncates the output and writes output to stderr.</li> <li><code>dryrun</code> echos everything after it to <code>stderr</code> if <code>DRYRUN</code> is set. Otherwise, it runs the command.</li> <li><code>waitforjobs</code> watches the children of the current process and sleeps until there are fewer than 10 running.</li> <li><code>iffmain</code> generates bash code. It runs <code>set -euo pipefail</code> and the specified function only if file is not sourced -- e.g. <code>bash tat2_all.bash</code> or <code>./tat2_all.bash</code> <sup>3</sup></li> <li><code>warn</code> sends a message to <code>stderr</code> so it doesn't get included in any eval/capture -- <code>a=$(warn 'oh no'; echo 'yes')</code> yields <code>a=\"yes\"</code></li> </ol>"},{"location":"shell/#in-use","title":"In Use","text":"<p>If we have files like <pre><code>sub-1\n\u2514\u2500\u2500 ses-1\n \u00a0\u00a0 \u2514\u2500\u2500 func\n \u00a0\u00a0     \u251c\u2500\u2500 sub-1_ses-1_func_task-rest_bold.nii.gz\n \u00a0\u00a0     \u2514\u2500\u2500 sub-1_ses-1_func_task-rest_motion.txt\n</code></pre></p> <p>If we set <code>DRYRUN</code>, we'll see what the script would do: a \"dry run\". <pre><code>DRYRUN=1 ./tat2_all.bash\n</code></pre></p> <pre><code>#       1\n#       1\n#       1\n#       0\n#       1 # (1)\n# would be written to sub-1/ses-1/func/sub-1_ses-1_func_task-rest_fdcen.1D  # (2)\ntat2 sub-1/ses-1/func/sub-1_ses-1_func_task-rest_bold.nii.gz -censor sub-1/ses-1/func/sub-1_ses-1_func_task-rest_fdcen.1D\n# (3)!\n</code></pre> <ol> <li>output of <code>fd_calc</code>, <code>drytee</code> truncated, prefixed with <code>#\\t</code> and sent to stderr</li> <li><code>drytee</code> also mentions what file it would have created. This file still does not exist</li> <li><code>dryrun</code> shows but does not run the <code>tat2</code> command.</li> </ol>"},{"location":"shell/#sourcedebug","title":"Source/Debug","text":"<p>Because the bash file is only functions and <code>iffmain</code> does not run if sourced, we can debug with <code>source</code>. Here we'll run the <code>create_censor</code> function defined in <code>tat2_all.bash</code> to check that it does what we expect.</p> <pre><code>source tat2_all.bash\ncreate_censor sub-1/ses-1/func/sub-1_ses-1_func_task-rest_bold.nii.gz\ncat sub-1/ses-1/func/sub-1_ses-1_func_task-rest_fdcen.1D\n</code></pre> sub-1/ses-1/func/sub-1_ses-1_func_task-rest_fdcen.1D<pre><code>1\n1\n1\n0\n1\n1\n</code></pre> <ol> <li> <p>\"dryrun\"'s name is taken from the rsync \"--dryrun\" option. <code>perl-rename</code> alias <code>--dry-run</code> with <code>--just-print</code> \u21a9</p> </li> <li> <p><code>set -e</code> \"exit on an error\" is especially disruptive.  One typo command and your interactive shell closes itself.\u00a0\u21a9</p> </li> <li> <p>sourcing a shell script is useful for running same-file tests with bats and/or embedding the current file in other scripts to reuse function definitions. See [Sourcing][#sourcing]\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"tat2/","title":"Time averaged T2*","text":"<p><code>tat2</code> (code) wraps around <code>3dROIstats</code>, <code>3dcalc</code>, and <code>3dTstat</code> to reduce 4D EPI BOLD data to a per-voxel (3D) measure (<code>nT2*</code>) that is inversely related to iron concentration.</p> <p>This is used in</p> <ul> <li>Contributions of dopamine-related basal ganglia neurophysiology to the developmental effects of incentives on inhibitory control</li> <li>In vivo evidence of neurophysiological maturation of the human adolescent striatum</li> </ul> <p>And is initially from Predicting Individuals' Learning Success from Patterns of Pre-Learning MRI Activity</p> <p>The normalization is within the brain for each time point. Not all voxels in the volume, but only those that belong to the brain mask. Then we normalize so that the sum of all brain voxels is some fixed number, e.g., 10000. The number doesn't really matter.</p> <p>Also see Relative Concentration of Brain Iron (rcFe).</p>"},{"location":"tat2/#setup","title":"Setup","text":"<p>See the setup instructions for all of lncdtools. Breifely, <code>git clone https://github.com/lncd/lncdtools</code> and add the new directory to your path.</p> <p>The raw tat2 script can stand alone, but will uses other lncdtools scripts if avaiable -- namely <code>gitver</code>. It's also a lot easier to fetch and track updates when the script is within source control. You get that when you clone the repo.</p>"},{"location":"tat2/#usage","title":"Usage","text":"<p>see <code>tat2 --help</code></p>"},{"location":"tat2/#simple","title":"Simple","text":"<pre><code>tat2 -output derive/tat2.nii.gz func/*preproc_bold.nii.gz\n</code></pre>"},{"location":"tat2/#with-options-and-relative-paths","title":"With options and relative paths","text":"<pre><code>sub_ses=\"sub-01/ses-01\"\ncensor_regex='s/.*func\\/(.*)-preproc_bold.nii.gz/censor_files\\/\\1\\/preproc_censor-fd0.3.1D/'\ntat2 \\\n    -output \"deriv/$sub_ses/func/${sub_ses//\\//_}_space-MNI152NLin2009cAsym_tat2.nii.gz\" \\\n    -mask_rel 's/preproc_bold.nii.gz/brain_mask.nii.gz/' \\\n    -censor_rel \"$censor_regex\" \\\n    -median_time \\\n    -median_vol \\\n    -no_voxscale \\\n    -verbose \\\n    deriv/$sub_ses/func/*space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n</code></pre>"},{"location":"tat2/#preprocessing","title":"Preprocessing","text":"<p>We slice-time and motion correction, skull strip, despiking (wavelet), and warp to MNI before running <code>tat2</code>. Notably, smoothing is not included in datasets input to <code>tat2</code>.</p>"},{"location":"tat2/#pipeline","title":"Pipeline","text":""},{"location":"tat2/#example","title":"Example","text":"<p>Example mean tat2 images <sup>1</sup></p> <p></p>"},{"location":"tat2/#comparisons","title":"Comparisons","text":"<p>Permutation of <code>tat2</code> calls were compared against R2 acquisitions. <code>-vol_median</code> is likely the appropriate normalization.  </p>"},{"location":"tat2/#correlation-with-r2","title":"Correlation with R2","text":"<p><code>tat2</code> is negatively correlated <code>R2*</code> <sup>2</sup></p> <p> </p> <ol> <li> <p>from <code>/Volumes/Hera/Datasets/ABCD/TAT2/tat2_avg3797_med_voldisc.nii.gz</code> and <code>/Volumes/Hera/Projects/7TBrainMech/scripts/mri/tat2/mean_176.nii.gz</code> \u21a9</p> </li> <li> <p>from <code>/Volumes/Phillips/mMR_PETDA/scripts/tat2/multiverse</code> \u21a9</p> </li> </ol>"}]}